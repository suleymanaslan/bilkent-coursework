{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import imageio\n",
    "import glob\n",
    "from IPython import display\n",
    "\n",
    "seaborn.set()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "def read_datafile(file):\n",
    "    with open(file, 'r') as datafile:\n",
    "        contents = datafile.read()\n",
    "        data = np.array([line.split('\\t') for line in contents.split('\\n')], dtype=np.float64)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "def normalize_data(data, training_mean, training_std):\n",
    "    data = data - training_mean\n",
    "    data = data / training_std\n",
    "    return data\n",
    "\n",
    "train1_data = read_datafile('data/train1')\n",
    "train2_data = read_datafile('data/train2')\n",
    "print(f\"train1:{train1_data.shape}, train2:{train2_data.shape}\")\n",
    "\n",
    "train1_x, train1_y = train1_data.T\n",
    "train2_x, train2_y = train2_data.T\n",
    "\n",
    "mean_train1_x = np.mean(train1_x)\n",
    "std_train1_x = np.std(train1_x)\n",
    "\n",
    "mean_train1_y = np.mean(train1_y)\n",
    "std_train1_y = np.std(train1_y)\n",
    "\n",
    "mean_train2_x = np.mean(train2_x)\n",
    "std_train2_x = np.std(train2_x)\n",
    "\n",
    "mean_train2_y = np.mean(train2_y)\n",
    "std_train2_y = np.std(train2_y)\n",
    "\n",
    "train1_x = normalize_data(train1_x, mean_train1_x, std_train1_x)\n",
    "train1_y = normalize_data(train1_y, mean_train1_y, std_train1_y)\n",
    "train2_x = normalize_data(train2_x, mean_train2_x, std_train2_x)\n",
    "train2_y = normalize_data(train2_y, mean_train2_y, std_train2_y)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(train1_x, train1_y)\n",
    "plt.title('train1 data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(train2_x, train2_y)\n",
    "plt.title('train2 data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "train1_nb_examples = train1_x.shape[0]\n",
    "train2_nb_examples = train2_x.shape[0]\n",
    "\n",
    "test1_data = read_datafile('data/test1')\n",
    "test2_data = read_datafile('data/test2')\n",
    "print(f\"test1:{test1_data.shape}, test2:{test2_data.shape}\")\n",
    "\n",
    "test1_x, test1_y = test1_data.T\n",
    "test2_x, test2_y = test2_data.T\n",
    "\n",
    "test1_x = normalize_data(test1_x, mean_train1_x, std_train1_x)\n",
    "test1_y = normalize_data(test1_y, mean_train1_y, std_train1_y)\n",
    "test2_x = normalize_data(test2_x, mean_train2_x, std_train2_x)\n",
    "test2_y = normalize_data(test2_y, mean_train2_y, std_train2_y)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(test1_x, test1_y)\n",
    "plt.title('test1 data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(test2_x, test2_y)\n",
    "plt.title('test2 data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "test1_nb_examples = test1_x.shape[0]\n",
    "test2_nb_examples = test2_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_train1_x = np.min(train1_x)\n",
    "max_train1_x = np.max(train1_x)\n",
    "train1_uniform_x_samples = np.linspace(min_train1_x, max_train1_x, train1_nb_examples)\n",
    "\n",
    "min_train2_x = np.min(train2_x)\n",
    "max_train2_x = np.max(train2_x)\n",
    "train2_uniform_x_samples = np.linspace(min_train2_x, max_train2_x, train2_nb_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressorANN:\n",
    "    def __init__(self, input_dim=1, output_dim=1):\n",
    "        self.w1 = np.array(np.random.normal(size=(output_dim, input_dim)))\n",
    "        self.b1 = np.full((output_dim, 1), 0.01)\n",
    "    \n",
    "    def set_weights(self, new_weights):\n",
    "        self.w1 = new_weights\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.x = inputs\n",
    "        self.nb_of_input = self.x.shape[0]\n",
    "        \n",
    "        self.wpb = np.matmul(self.w1, self.x) + self.b1\n",
    "        return self.wpb\n",
    "    \n",
    "    def loss(self, yt):\n",
    "        self.err = yt.reshape((self.nb_of_input, 1)) - self.wpb.reshape((self.nb_of_input, 1))\n",
    "        self.errsqr = self.err**2\n",
    "        return self.errsqr\n",
    "    \n",
    "    def backward(self, learning_rate):\n",
    "        derr = (2 * self.err)\n",
    "        dyt = 1 * derr\n",
    "        self.dwpb = -1 * derr\n",
    "        dwmx = (1 * self.dwpb).reshape((self.nb_of_input, 1, 1))\n",
    "        db1 = 1 * self.dwpb\n",
    "        dw1 = np.matmul(dwmx, np.transpose(self.x, axes=(0, 2, 1)))\n",
    "        dx = np.matmul(np.transpose(self.w1), dwmx) \n",
    "        \n",
    "        self.w1 = self.w1 - learning_rate * np.mean(dw1, axis=0)\n",
    "        self.b1 = self.b1 - learning_rate * np.mean(db1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerANN:\n",
    "    def __init__(self, units, input_dim=1, output_dim=1, \n",
    "                 activation_function=\"relu\", \n",
    "                 loss_function=\"mse\", \n",
    "                 use_momentum=False, momentum_factor=0.9):\n",
    "        self.w1 = np.random.normal(size=(units, input_dim)) * np.sqrt(2.0/input_dim)\n",
    "        self.b1 = np.full((units, 1), 0.01)\n",
    "                                                                                     \n",
    "        self.w2 = np.random.normal(size=(output_dim, units)) * np.sqrt(2.0/units)\n",
    "        self.b2 = np.full((output_dim, 1), 0.01)\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.use_momentum = use_momentum\n",
    "        self.momentum_factor = momentum_factor\n",
    "        self.v1, self.v2, self.v3, self.v4 = 0, 0, 0, 0\n",
    "        \n",
    "    def set_weights_1(self, new_weights):\n",
    "        self.w1 = new_weights\n",
    "\n",
    "    def set_weights_2(self, new_weights):\n",
    "        self.w2 = new_weights\n",
    "    \n",
    "    def forward(self, inputs):        \n",
    "        self.x = inputs\n",
    "        self.nb_of_input = self.x.shape[0]\n",
    "        \n",
    "        self.wmx = np.matmul(self.w1, self.x)\n",
    "        self.wpb = self.wmx + self.b1\n",
    "        self.act = self.hidden_activation()\n",
    "        self.wmr = np.matmul(self.w2, self.act);\n",
    "        self.wpb2 = self.wmr + self.b2\n",
    "        return self.wpb2\n",
    "    \n",
    "    def hidden_activation(self):\n",
    "        if self.activation_function == \"relu\":\n",
    "            self.sigmoid = None\n",
    "            self.lrelu = None\n",
    "            self.relu = np.maximum(self.wpb, np.zeros_like(self.wpb))\n",
    "            return self.relu\n",
    "        elif self.activation_function == \"sigmoid\":\n",
    "            self.relu = None\n",
    "            self.lrelu = None\n",
    "            self.sigmoid = 1.0 / (1 + np.exp(-self.wpb))\n",
    "            return self.sigmoid\n",
    "        elif self.activation_function == \"lrelu\":\n",
    "            self.relu = None\n",
    "            self.sigmoid = None\n",
    "            self.lrelu_cons = 0.01\n",
    "            self.lrelu = np.where(self.wpb > 0, self.wpb, self.lrelu_cons * self.wpb)\n",
    "            return self.lrelu\n",
    "    \n",
    "    def loss(self, yt):\n",
    "        if self.loss_function == \"mse\":\n",
    "            self.abserr = None\n",
    "            self.err = yt.reshape((self.nb_of_input, 1)) - self.wpb2.reshape((self.nb_of_input, 1))\n",
    "            self.errsqr = self.err**2\n",
    "            return self.errsqr\n",
    "        elif self.loss_function == \"mae\":\n",
    "            self.errsqr = None\n",
    "            self.err = yt.reshape((self.nb_of_input, 1)) - self.wpb2.reshape((self.nb_of_input, 1))\n",
    "            self.abserr = np.abs(self.err)\n",
    "            return self.abserr\n",
    "    \n",
    "    def backward_loss(self):\n",
    "        if self.loss_function == \"mse\":\n",
    "            derr = (2 * self.err)\n",
    "            dyt = 1 * derr\n",
    "            self.dwpb2 = -1 * derr\n",
    "            return self.dwpb2\n",
    "        elif self.loss_function == \"mae\":\n",
    "            derr = np.where(self.err > 0, 1, -1)\n",
    "            dyt = 1 * derr\n",
    "            self.dwpb2 = -1 * derr\n",
    "            return self.dwpb2\n",
    "    \n",
    "    def backward_hidden_activation(self):\n",
    "        if self.activation_function == \"relu\":\n",
    "            self.dwpb = np.where(self.wpb > 0, 1 * self.dact, 0)\n",
    "            return self.dwpb\n",
    "        elif self.activation_function == \"sigmoid\":\n",
    "            self.dwpb = ((1 - self.sigmoid) * self.sigmoid) * self.dact\n",
    "            return self.dwpb\n",
    "        elif self.activation_function == \"lrelu\":\n",
    "            self.dwpb = np.where(self.wpb > 0, 1 * self.dact, self.lrelu_cons * self.dact)\n",
    "            return self.dwpb\n",
    "    \n",
    "    def backward(self, learning_rate):\n",
    "        self.dwpb2 = self.backward_loss()\n",
    "        \n",
    "        dwmr = (1 * self.dwpb2).reshape((self.nb_of_input, 1, 1))\n",
    "        db2 = 1 * self.dwpb2\n",
    "        dw2 = np.matmul(dwmr, np.transpose(self.act, axes=(0, 2, 1)))\n",
    "        self.dact = np.matmul(np.transpose(self.w2), dwmr)\n",
    "        \n",
    "        self.dwpb = self.backward_hidden_activation()\n",
    "        \n",
    "        dwmx = 1 * self.dwpb\n",
    "        db1 = 1 * self.dwpb\n",
    "        \n",
    "        dw1 = np.matmul(dwmx, np.transpose(self.x, axes=(0, 2, 1)))\n",
    "        dx = np.matmul(np.transpose(self.w1), dwmx)\n",
    "\n",
    "        if self.use_momentum:\n",
    "            self.v1 = self.momentum_factor * self.v1 - learning_rate * np.mean(dw1, axis=0)\n",
    "            self.w1 = self.w1 + self.v1\n",
    "            self.v2 = self.momentum_factor * self.v2 - learning_rate * np.mean(db1, axis=0)\n",
    "            self.b1 = self.b1 + self.v2\n",
    "            self.v3 = self.momentum_factor * self.v3 - learning_rate * np.mean(dw2, axis=0)\n",
    "            self.w2 = self.w2 + self.v3\n",
    "            self.v4 = self.momentum_factor * self.v4 - learning_rate * np.mean(db2, axis=0)\n",
    "            self.b2 = self.b2 + self.v4\n",
    "        else:\n",
    "            self.w1 = self.w1 - learning_rate * np.mean(dw1, axis=0)\n",
    "            self.b1 = self.b1 - learning_rate * np.mean(db1, axis=0)\n",
    "            self.w2 = self.w2 - learning_rate * np.mean(dw2, axis=0)\n",
    "            self.b2 = self.b2 - learning_rate * np.mean(db2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor_ann = LinearRegressorANN()\n",
    "lra_output = linear_regressor_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(train1_x, train1_y)\n",
    "plt.plot(train1_uniform_x_samples, lra_output.reshape((train1_nb_examples, 1)), linewidth=3)\n",
    "plt.title('Linear regressor ANN')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "nb_of_hiddenunits = 2\n",
    "two_layer_ann = TwoLayerANN(nb_of_hiddenunits)\n",
    "tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape(train1_nb_examples, INPUT_DIM, OUTPUT_DIM))\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(train1_x, train1_y)\n",
    "plt.plot(train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), linewidth=3)\n",
    "plt.title('Two layer ANN')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear regressor ANN loss:{np.mean(linear_regressor_ann.loss(train1_y)):.2f}\")\n",
    "print(f\"Two layer ANN loss:{np.mean(two_layer_ann.loss(train1_y)):.2f}\")\n",
    "mean_regressor_loss = (train1_y.reshape((train1_nb_examples, 1)) - np.full((train1_nb_examples, 1), np.mean(train1_y)))**2\n",
    "print(f\"Mean regressor loss:{np.mean(mean_regressor_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor_ann = LinearRegressorANN() # reset network\n",
    "min_lra_loss = np.inf\n",
    "random_weights = np.arange(-10,11)\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "for i in random_weights:\n",
    "    linear_regressor_ann.set_weights(i.reshape((OUTPUT_DIM, INPUT_DIM)))\n",
    "    lra_output = linear_regressor_ann.forward(train1_x.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    lra_loss = np.mean(linear_regressor_ann.loss(train1_y))\n",
    "    if lra_loss < min_lra_loss:\n",
    "        min_lra_loss = lra_loss\n",
    "    plt.scatter(i, lra_loss, color=\"blue\")\n",
    "plt.title('Loss for Linear regressor ANN')\n",
    "plt.xlabel('weights')\n",
    "plt.ylabel('loss')\n",
    "print(f\"Minimum loss:{min_lra_loss:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 2\n",
    "two_layer_ann = TwoLayerANN(nb_of_hiddenunits) # reset network\n",
    "random_weights = np.arange(-10,11)\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in random_weights:\n",
    "    for j in random_weights:\n",
    "        two_layer_ann.set_weights_1(np.array([i, j]).reshape((nb_of_hiddenunits, INPUT_DIM)))\n",
    "        tla_output = two_layer_ann.forward(train1_x.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train1_y))\n",
    "        ax.scatter(i, j, tla_loss, color=\"blue\")\n",
    "plt.title('Loss for Two Layer ANN')\n",
    "ax.set_xlabel('weights_1')\n",
    "ax.set_ylabel('weights_1')\n",
    "ax.set_zlabel('loss')\n",
    "plt.show()\n",
    "\n",
    "two_layer_ann = TwoLayerANN(nb_of_hiddenunits) # reset network\n",
    "random_weights = np.arange(-10,11)\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in random_weights:\n",
    "    for j in random_weights:\n",
    "        two_layer_ann.set_weights_2(np.array([i, j]).reshape((OUTPUT_DIM, nb_of_hiddenunits)))\n",
    "        tla_output = two_layer_ann.forward(train1_x.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train1_y))\n",
    "        ax.scatter(i, j, tla_loss, color=\"blue\")\n",
    "plt.title('Loss for Two Layer ANN')\n",
    "ax.set_xlabel('weights_2')\n",
    "ax.set_ylabel('weights_2')\n",
    "ax.set_zlabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor_ann = LinearRegressorANN()\n",
    "two_layer_ann = TwoLayerANN(nb_of_hiddenunits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "linear_regressor_ann = LinearRegressorANN() # reset network\n",
    "learning_rate = 6e-2\n",
    "nb_of_epochs = 20\n",
    "batch_size = 30\n",
    "\n",
    "fig = plt.figure()\n",
    "min_lra_loss = np.inf\n",
    "for epoch in range(nb_of_epochs):\n",
    "    for i in range(train1_nb_examples//batch_size):\n",
    "        linear_regressor_ann.forward(train1_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "        linear_regressor_ann.loss(train1_y[i*batch_size:i*batch_size+batch_size])\n",
    "        linear_regressor_ann.backward(learning_rate)\n",
    "    lra_output = linear_regressor_ann.forward(train1_x.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    lra_loss = np.mean(linear_regressor_ann.loss(train1_y))\n",
    "    print(f\"Epoch:{epoch+1}, Linear regressor ANN loss:{lra_loss:.4f}\")\n",
    "    plt.scatter(train1_x, train1_y)\n",
    "    lra_output = linear_regressor_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    plt.plot(train1_uniform_x_samples, lra_output.reshape((train1_nb_examples, 1)), linewidth=3)\n",
    "    plt.title(f'Linear regressor ANN, Epoch:{epoch+1}, Training Set, Loss:{lra_loss:.4f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.savefig(f'gif/lra/lra_{epoch+1:03d}.png')\n",
    "    plt.close()\n",
    "    if min_lra_loss - lra_loss > 1e-5:\n",
    "        min_lra_loss = lra_loss\n",
    "    else:\n",
    "        print(\"Stopped training\")\n",
    "        plt.scatter(train1_x, train1_y)\n",
    "        lra_output = linear_regressor_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        plt.plot(train1_uniform_x_samples, lra_output.reshape((train1_nb_examples, 1)), linewidth=3)\n",
    "        plt.title(f'Linear regressor ANN, Epoch:{epoch+1}, Training Set, Loss:{lra_loss:.4f}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.savefig('output/lra_train.png')\n",
    "        plt.close()\n",
    "        break\n",
    "\n",
    "anim_file = 'gif/lra_training.gif'\n",
    "\n",
    "frames = []\n",
    "filenames = glob.glob('gif/lra/lra_*.png')\n",
    "filenames = sorted(filenames)\n",
    "for i, filename in enumerate(filenames):\n",
    "    frames.append(imageio.imread(filename))\n",
    "for i in range(10):\n",
    "    frames.append(imageio.imread(filename))\n",
    "    \n",
    "imageio.mimsave(anim_file, frames, 'GIF', fps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename='gif/lra_training.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lra_output = linear_regressor_ann.forward(train1_x.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "lra_loss = np.mean(linear_regressor_ann.loss(train1_y))\n",
    "lra_loss_std = np.std(linear_regressor_ann.loss(train1_y))\n",
    "print(f\"Linear regressor ANN, training set loss:{lra_loss:.4f}, std:{lra_loss_std:.4f}\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(train1_x, train1_y)\n",
    "lra_output = linear_regressor_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train1_uniform_x_samples, lra_output.reshape((train1_nb_examples, 1)), linewidth=3)\n",
    "plt.title(f'Linear regressor ANN, Training Set, Loss:{np.mean(lra_loss):.4f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('output/lra_train_curve.png')\n",
    "plt.show()\n",
    "\n",
    "lra_output = linear_regressor_ann.forward(test1_x.reshape((test1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "lra_loss = np.mean(linear_regressor_ann.loss(test1_y))\n",
    "lra_loss_std = np.std(linear_regressor_ann.loss(test1_y))\n",
    "print(f\"Linear regressor ANN, test set loss:{lra_loss:.4f}, std:{lra_loss_std:.4f}\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(test1_x, test1_y)\n",
    "lra_output = linear_regressor_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train1_uniform_x_samples, lra_output.reshape((train1_nb_examples, 1)), linewidth=3)\n",
    "plt.title(f'Linear regressor ANN, Test Set, Loss:{np.mean(lra_loss):.4f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('output/lra_test_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_config = {2: 8e-3, 4: 1e-2, 8: 1e-2, 16: 5e-3}\n",
    "epoch_config = {2: 5500, 4: 8000, 8: 7500, 16: 9000}\n",
    "batchsize_config = {2: 2, 4: 2, 8: 2, 16: 3}\n",
    "activation_config = {2: \"sigmoid\", 4: \"sigmoid\", 8: \"sigmoid\", 16: \"sigmoid\"}\n",
    "loss_config = {2: \"mse\", 4: \"mse\", 8: \"mse\", 16: \"mse\"}\n",
    "momentum_config = {2: 0.75, 4: 0.75, 8: 0.9, 16: 0.6} # use 0 for no momentum\n",
    "stop_loss_config = {2: 0.05795, 4: 0.02025, 8: 0.02045, 16: 0.02065}\n",
    "plot_color = {2: \"red\", 4: \"cyan\", 8: \"magenta\", 16: \"black\"}\n",
    "\n",
    "trained_nets = []\n",
    "\n",
    "for nb_of_hiddenunits in (2, 4, 8, 16):\n",
    "    np.random.seed(550)\n",
    "    learning_rate = lr_config[nb_of_hiddenunits]\n",
    "    nb_of_epochs = epoch_config[nb_of_hiddenunits]\n",
    "    batch_size = batchsize_config[nb_of_hiddenunits]\n",
    "\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation_config[nb_of_hiddenunits], \n",
    "                                loss_function=loss_config[nb_of_hiddenunits], \n",
    "                                use_momentum=True, momentum_factor=momentum_config[nb_of_hiddenunits]) # reset network\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train1_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train1_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train1_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train1_x.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train1_y))\n",
    "        if epoch == 0 or (epoch+1) % 500 == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train1_x, train1_y)\n",
    "            tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), \n",
    "                     color=plot_color[nb_of_hiddenunits], linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/tla_{nb_of_hiddenunits}/tla_{epoch+1:04d}.png')\n",
    "            plt.close()\n",
    "        if tla_loss < stop_loss_config[nb_of_hiddenunits]:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train1_x, train1_y)\n",
    "            tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), \n",
    "                     color=plot_color[nb_of_hiddenunits], linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla_{nb_of_hiddenunits}_train.png')\n",
    "            plt.close()\n",
    "            break\n",
    "            \n",
    "    anim_file = f'gif/tla_{nb_of_hiddenunits}_training.gif'\n",
    "    \n",
    "    frames = []\n",
    "    filenames = glob.glob(f'gif/tla_{nb_of_hiddenunits}/tla_*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    \n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=4)\n",
    "    \n",
    "    trained_nets.append(two_layer_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 2\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 4\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 16\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_hidden_units = [2, 4, 8, 16]\n",
    "for i in range(4):\n",
    "    two_layer_ann = trained_nets[i]\n",
    "    tla_output = two_layer_ann.forward(train1_x.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    tla_loss = np.mean(two_layer_ann.loss(train1_y))\n",
    "    tla_loss_std = np.std(two_layer_ann.loss(train1_y))\n",
    "    print(f\"Two layer ANN, {ann_hidden_units[i]} units, training set loss:{tla_loss:.4f}, std:{tla_loss_std:.4f}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.scatter(train1_x, train1_y)\n",
    "    tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    plt.plot(train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), \n",
    "             color=plot_color[ann_hidden_units[i]], linewidth=3)\n",
    "    plt.title(f'Two layer ANN, {ann_hidden_units[i]} units, Training Set, Loss:{np.mean(tla_loss):.4f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.savefig(f'output/tla_{ann_hidden_units[i]}_train_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "    tla_output = two_layer_ann.forward(test1_x.reshape((test1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    tla_loss = np.mean(two_layer_ann.loss(test1_y))\n",
    "    tla_loss_std = np.std(two_layer_ann.loss(test1_y))\n",
    "    print(f\"Two layer ANN, {ann_hidden_units[i]} units, test set loss:{tla_loss:.4f}, std:{tla_loss_std:.4f}\")\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.scatter(test1_x, test1_y)\n",
    "    tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    plt.plot(train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), \n",
    "             color=plot_color[ann_hidden_units[i]], linewidth=3)\n",
    "    plt.title(f'Two layer ANN, {ann_hidden_units[i]} units, Test Set, Loss:{np.mean(tla_loss):.4f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.savefig(f'output/tla_{ann_hidden_units[i]}_test_curve.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(train1_x, train1_y)\n",
    "\n",
    "two_layer_ann = trained_nets[0]\n",
    "tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), label='2 Units', \n",
    "             color=plot_color[2], linewidth=3)\n",
    "\n",
    "two_layer_ann = trained_nets[1]\n",
    "tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), label='4 Units', \n",
    "             color=plot_color[4], linewidth=6)\n",
    "\n",
    "two_layer_ann = trained_nets[2]\n",
    "tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), label='8 Units', \n",
    "             color=plot_color[8], linewidth=3)\n",
    "\n",
    "two_layer_ann = trained_nets[3]\n",
    "tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), label='16 Units', \n",
    "             color=plot_color[16], linewidth=3)\n",
    "\n",
    "leg = plt.legend(loc='upper left')\n",
    "\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(3)\n",
    "\n",
    "plt.title(f'Two layer ANNs with different number of hidden units')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('output/tla_all_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "linear_regressor_ann = LinearRegressorANN() # reset network\n",
    "learning_rate = 1.5e-1\n",
    "nb_of_epochs = 20\n",
    "batch_size = 229\n",
    "\n",
    "fig = plt.figure()\n",
    "min_lra_loss = np.inf\n",
    "for epoch in range(nb_of_epochs):\n",
    "    for i in range(train2_nb_examples//batch_size):\n",
    "        linear_regressor_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "        linear_regressor_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "        linear_regressor_ann.backward(learning_rate)\n",
    "    lra_output = linear_regressor_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    lra_loss = np.mean(linear_regressor_ann.loss(train2_y))\n",
    "    print(f\"Epoch:{epoch+1}, Linear regressor ANN loss:{lra_loss:.4f}\")\n",
    "    plt.scatter(train2_x, train2_y)\n",
    "    lra_output = linear_regressor_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    plt.plot(train2_uniform_x_samples, lra_output.reshape((train2_nb_examples, 1)), linewidth=3)\n",
    "    plt.title(f'Linear regressor ANN, Epoch:{epoch+1}, Training Set, Loss:{lra_loss:.4f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.savefig(f'gif/lra_2/lra_{epoch+1:03d}.png')\n",
    "    plt.close()\n",
    "    if min_lra_loss - lra_loss > 1e-5:\n",
    "        min_lra_loss = lra_loss\n",
    "    else:\n",
    "        print(\"Stopped training\")\n",
    "        plt.scatter(train2_x, train2_y)\n",
    "        lra_output = linear_regressor_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        plt.plot(train2_uniform_x_samples, lra_output.reshape((train2_nb_examples, 1)), linewidth=3)\n",
    "        plt.title(f'Linear regressor ANN, Epoch:{epoch+1}, Training Set, Loss:{lra_loss:.4f}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.savefig('output/lra_train_2.png')\n",
    "        plt.close()\n",
    "        break\n",
    "\n",
    "anim_file = 'gif/lra_training_2.gif'\n",
    "\n",
    "frames = []\n",
    "filenames = glob.glob('gif/lra_2/lra_*.png')\n",
    "filenames = sorted(filenames)\n",
    "for i, filename in enumerate(filenames):\n",
    "    frames.append(imageio.imread(filename))\n",
    "for i in range(10):\n",
    "    frames.append(imageio.imread(filename))\n",
    "    \n",
    "imageio.mimsave(anim_file, frames, 'GIF', fps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'gif/lra_training_2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lra_output = linear_regressor_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "lra_loss = np.mean(linear_regressor_ann.loss(train2_y))\n",
    "lra_loss_std = np.std(linear_regressor_ann.loss(train2_y))\n",
    "print(f\"Linear regressor ANN, training set loss:{lra_loss:.4f}, std:{lra_loss_std:.4f}\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(train2_x, train2_y)\n",
    "lra_output = linear_regressor_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, lra_output.reshape((train2_nb_examples, 1)), linewidth=3)\n",
    "plt.title(f'Linear regressor ANN, Training Set, Loss:{np.mean(lra_loss):.4f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('output/lra_train_curve_2.png')\n",
    "plt.show()\n",
    "\n",
    "lra_output = linear_regressor_ann.forward(test2_x.reshape((test2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "lra_loss = np.mean(linear_regressor_ann.loss(test2_y))\n",
    "lra_loss_std = np.std(linear_regressor_ann.loss(test2_y))\n",
    "print(f\"Linear regressor ANN, test set loss:{lra_loss:.4f}, std:{lra_loss_std:.4f}\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(test2_x, test2_y)\n",
    "lra_output = linear_regressor_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, lra_output.reshape((train2_nb_examples, 1)), linewidth=3)\n",
    "plt.title(f'Linear regressor ANN, Test Set, Loss:{np.mean(lra_loss):.4f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('output/lra_test_curve_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_config = {2: 9e-3, 4: 9e-3, 8: 1e-2, 16: 9e-3}\n",
    "epoch_config = {2: 7500, 4: 6500, 8: 8000, 16: 30000}\n",
    "batchsize_config = {2: 3, 4: 3, 8: 2, 16: 2}\n",
    "activation_config = {2: \"sigmoid\", 4: \"sigmoid\", 8: \"sigmoid\", 16: \"sigmoid\"}\n",
    "loss_config = {2: \"mse\", 4: \"mse\", 8: \"mse\", 16: \"mse\"}\n",
    "momentum_config = {2: 0.4, 4: 0.4, 8: 0.5, 16: 0.3} # use 0 for no momentum\n",
    "stop_loss_config = {2: 0.28005, 4: 0.14305, 8: 0.05975, 16: 0.05915}\n",
    "plot_color = {2: \"red\", 4: \"cyan\", 8: \"magenta\", 16: \"black\"}\n",
    "\n",
    "trained_nets_2 = []\n",
    "\n",
    "for nb_of_hiddenunits in (2, 4, 8, 16):\n",
    "    np.random.seed(550)\n",
    "    learning_rate = lr_config[nb_of_hiddenunits]\n",
    "    nb_of_epochs = epoch_config[nb_of_hiddenunits]\n",
    "    batch_size = batchsize_config[nb_of_hiddenunits]\n",
    "\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation_config[nb_of_hiddenunits], \n",
    "                                loss_function=loss_config[nb_of_hiddenunits], \n",
    "                                use_momentum=True, momentum_factor=momentum_config[nb_of_hiddenunits]) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (1500 if nb_of_hiddenunits == 16 else 500) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), \n",
    "                     color=plot_color[nb_of_hiddenunits], linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/tla_{nb_of_hiddenunits}_2/tla_{epoch+1:05d}.png')\n",
    "            plt.close()\n",
    "        if tla_loss < stop_loss_config[nb_of_hiddenunits]:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), \n",
    "                     color=plot_color[nb_of_hiddenunits], linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2.png')\n",
    "            plt.close()\n",
    "            break\n",
    "    anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "    \n",
    "    frames = []\n",
    "    filenames = glob.glob(f'gif/tla_{nb_of_hiddenunits}_2/tla_*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    \n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "    trained_nets_2.append(two_layer_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 2\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 4\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 16\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_hidden_units = [2, 4, 8, 16]\n",
    "for i in range(4):\n",
    "    two_layer_ann = trained_nets_2[i]\n",
    "    tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "    tla_loss_std = np.std(two_layer_ann.loss(train2_y))\n",
    "    print(f\"Two layer ANN, {ann_hidden_units[i]} units, training set loss:{tla_loss:.4f}, std:{tla_loss_std:.4f}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.scatter(train2_x, train2_y)\n",
    "    tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), \n",
    "             color=plot_color[ann_hidden_units[i]], linewidth=3)\n",
    "    plt.title(f'Two layer ANN, {ann_hidden_units[i]} units, Training Set, Loss:{np.mean(tla_loss):.4f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.savefig(f'output/tla_{ann_hidden_units[i]}_train_curve_2.png')\n",
    "    plt.show()\n",
    "\n",
    "    tla_output = two_layer_ann.forward(test2_x.reshape((test2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    tla_loss = np.mean(two_layer_ann.loss(test2_y))\n",
    "    tla_loss_std = np.std(two_layer_ann.loss(test2_y))\n",
    "    print(f\"Two layer ANN, {ann_hidden_units[i]} units, test set loss:{tla_loss:.4f}, std:{tla_loss_std:.4f}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.scatter(test2_x, test2_y)\n",
    "    tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), \n",
    "             color=plot_color[ann_hidden_units[i]], linewidth=3)\n",
    "    plt.title(f'Two layer ANN, {ann_hidden_units[i]} units, Test Set, Loss:{np.mean(tla_loss):.4f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.savefig(f'output/tla_{ann_hidden_units[i]}_test_curve_2.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(train2_x, train2_y)\n",
    "\n",
    "two_layer_ann = trained_nets_2[0]\n",
    "tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), label='2 Units', \n",
    "             color=plot_color[2], linewidth=3)\n",
    "\n",
    "two_layer_ann = trained_nets_2[1]\n",
    "tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), label='4 Units', \n",
    "             color=plot_color[4], linewidth=3)\n",
    "\n",
    "two_layer_ann = trained_nets_2[2]\n",
    "tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), label='8 Units', \n",
    "             color=plot_color[8], linewidth=3)\n",
    "\n",
    "two_layer_ann = trained_nets_2[3]\n",
    "tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), label='16 Units', \n",
    "             color=plot_color[16], linewidth=3)\n",
    "\n",
    "leg = plt.legend(loc='lower left')\n",
    "\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(3)\n",
    "\n",
    "plt.title(f'Two layer ANNs with different number of hidden units')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('output/tla_all_curves_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "lr = {0: 1, 1: 1e-1, 2: 1e-2, 3: 1e-3, 4: 1e-4}\n",
    "nb_epoch = {0: 1, 1: 500, 2: 2500, 3: 25000, 4: 225000}\n",
    "batch_size = 229\n",
    "activation = \"sigmoid\"\n",
    "loss = \"mse\"\n",
    "momentum = 0.99\n",
    "stop_loss = 0.12\n",
    "plot_color = \"magenta\"\n",
    "\n",
    "for j in lr:\n",
    "    np.random.seed(550)\n",
    "    learning_rate = lr[j]\n",
    "    nb_of_epochs = nb_epoch[j]\n",
    "\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation, \n",
    "                                loss_function=loss, \n",
    "                                use_momentum=True, momentum_factor=momentum) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units, LR:{learning_rate}\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (1 if j == 0 else (500 if j < 3 else (10000 if j == 3 else 30000))) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "        if tla_loss < stop_loss:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color,\n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_d_{j}.png')\n",
    "            plt.show()\n",
    "            break\n",
    "        if epoch == nb_of_epochs - 1:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color, \n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_d_{j}.png')\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "learning_rate = 1e-2\n",
    "nb_of_epochs = 300000\n",
    "batch_size = 229\n",
    "activation = \"sigmoid\"\n",
    "loss = \"mse\"\n",
    "mf = {0: 0, 1: 0.99}\n",
    "stop_loss = 0.12\n",
    "plot_color = \"magenta\"\n",
    "\n",
    "for j in mf:\n",
    "    np.random.seed(550)\n",
    "    momentum = mf[j]\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation, \n",
    "                                loss_function=loss, \n",
    "                                use_momentum=True, momentum_factor=momentum) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units, MF:{momentum}\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (40000 if j == 0 else 500) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "        if tla_loss < stop_loss:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color,\n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_e_{j}.png')\n",
    "            plt.show()\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "learning_rates = {0: 8e-3, 1: 1.5e-2}\n",
    "nb_of_epochs_config = {0: 3500, 1: 110000} \n",
    "batch_sizes = {0: 1, 1: 229}\n",
    "activation = \"sigmoid\"\n",
    "loss = \"mse\"\n",
    "mf = {0: 0.1, 1: 0.2}\n",
    "stop_loss = 0.125\n",
    "plot_color = \"magenta\"\n",
    "\n",
    "for j in batch_sizes:\n",
    "    np.random.seed(550)\n",
    "    learning_rate = learning_rates[j]\n",
    "    batch_size = batch_sizes[j]\n",
    "    momentum = mf[j]\n",
    "    nb_of_epochs = nb_of_epochs_config[j]\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation, \n",
    "                                loss_function=loss, \n",
    "                                use_momentum=True, momentum_factor=momentum) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units, BS:{batch_size}\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (500 if j == 0 else 20000) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "        if tla_loss < stop_loss:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color,\n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_f_{j}.png')\n",
    "            plt.show()\n",
    "            break\n",
    "        if epoch == nb_of_epochs - 1:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color, \n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_f_{j}.png')\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
