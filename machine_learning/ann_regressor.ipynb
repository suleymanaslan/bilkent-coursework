{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import imageio\n",
    "import glob\n",
    "from IPython import display\n",
    "\n",
    "seaborn.set()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import utils\n",
    "import neural_net\n",
    "import regression_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_x, train1_y, train2_x, train2_y, test1_x, test1_y, test2_x, test2_y = utils.read_dataset()\n",
    "train1_x, train1_y, train2_x, train2_y, test1_x, test1_y, test2_x, test2_y = utils.normalize_dataset(train1_x, train1_y, train2_x, train2_y, test1_x, test1_y, test2_x, test2_y)\n",
    "\n",
    "utils.plot_dataset(train1_x, train1_y, train2_x, train2_y, test1_x, test1_y, test2_x, test2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_nb_examples, train2_nb_examples, test1_nb_examples, test2_nb_examples = utils.get_shape(train1_x, train2_x, test1_x, test2_x)\n",
    "train1_uniform_x_samples, train2_uniform_x_samples = utils.get_uniform_samples(train1_x, train1_nb_examples, train2_x, train2_nb_examples)\n",
    "\n",
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "np.random.seed(550)\n",
    "linear_regressor_ann = neural_net.LinearRegressorANN()\n",
    "lra_output = linear_regressor_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "\n",
    "two_layer_ann = neural_net.TwoLayerANN(units=2)\n",
    "tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape(train1_nb_examples, INPUT_DIM, OUTPUT_DIM))\n",
    "\n",
    "utils.plot_output(train1_x, train1_y, train1_uniform_x_samples, lra_output.reshape((train1_nb_examples, 1)), 'Linear regressor ANN')\n",
    "utils.plot_output(train1_x, train1_y, train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), 'Two layer ANN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear regressor ANN loss:{np.mean(linear_regressor_ann.loss(train1_y)):.2f}\")\n",
    "print(f\"Two layer ANN loss:{np.mean(two_layer_ann.loss(train1_y)):.2f}\")\n",
    "print(f\"Mean regressor loss:{np.mean((train1_y.reshape((train1_nb_examples, 1)) - np.full((train1_nb_examples, 1), np.mean(train1_y)))**2):.2f}\")\n",
    "\n",
    "linear_regressor_ann = neural_net.LinearRegressorANN()\n",
    "regression_functions.plot_lra_random_weight_losses(linear_regressor_ann, INPUT_DIM, OUTPUT_DIM, train1_x, train1_nb_examples, train1_y)\n",
    "\n",
    "two_layer_ann = neural_net.TwoLayerANN(units=2)\n",
    "regression_functions.plot_tla_random_weight_losses(two_layer_ann, INPUT_DIM, OUTPUT_DIM, 2, train1_x, train1_nb_examples, train1_y, randomize_first_layer=True)\n",
    "\n",
    "two_layer_ann = neural_net.TwoLayerANN(units=2)\n",
    "regression_functions.plot_tla_random_weight_losses(two_layer_ann, INPUT_DIM, OUTPUT_DIM, 2, train1_x, train1_nb_examples, train1_y, randomize_first_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "linear_regressor_ann = neural_net.LinearRegressorANN()\n",
    "regression_functions.train_lra([6e-2, 20, 30], linear_regressor_ann, train1_x, train1_y, INPUT_DIM, OUTPUT_DIM, train1_nb_examples, train1_uniform_x_samples)\n",
    "anim_file = \"gif/lra_training.gif\"\n",
    "utils.create_animation(anim_file, 'gif/lra_*.png')\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lra_loss = regression_functions.evaluate_lra(linear_regressor_ann, train1_x, train1_nb_examples, INPUT_DIM, OUTPUT_DIM, train1_y, \"Training\")\n",
    "regression_functions.plot_lra_evaluation(linear_regressor_ann, train1_x, INPUT_DIM, OUTPUT_DIM, train1_y, \"Training\", \n",
    "                                         lra_loss, train1_uniform_x_samples, train1_nb_examples, \"train_curve\")\n",
    "\n",
    "lra_loss = regression_functions.evaluate_lra(linear_regressor_ann, test1_x, test1_nb_examples, INPUT_DIM, OUTPUT_DIM, test1_y, \"Test\")\n",
    "regression_functions.plot_lra_evaluation(linear_regressor_ann, test1_x, INPUT_DIM, OUTPUT_DIM, test1_y, \"Test\",\n",
    "                                         lra_loss, train1_uniform_x_samples, train1_nb_examples, \"test_curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained_nets, anim_files = regression_functions.train_tla(train1_x, train1_y, INPUT_DIM, OUTPUT_DIM, train1_nb_examples, train1_uniform_x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regression_functions.evaluate_tla(trained_nets, train1_x, train1_y, test1_x, test1_y, train1_nb_examples, test1_nb_examples, train1_uniform_x_samples, INPUT_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_functions.plot_tla_curves(trained_nets, train1_x, train1_y, train1_uniform_x_samples, train1_nb_examples, INPUT_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "linear_regressor_ann = neural_net.LinearRegressorANN()\n",
    "regression_functions.train_lra([1.5e-1, 20, 229], linear_regressor_ann, train2_x, train2_y, INPUT_DIM, OUTPUT_DIM, train2_nb_examples, train2_uniform_x_samples, label=\"train_2\")\n",
    "anim_file = 'gif/lra_training_2.gif'\n",
    "utils.create_animation(anim_file, 'gif/lra_*.png')\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lra_loss = regression_functions.evaluate_lra(linear_regressor_ann, train2_x, train2_nb_examples, INPUT_DIM, OUTPUT_DIM, train2_y, \"Training\")\n",
    "regression_functions.plot_lra_evaluation(linear_regressor_ann, train2_x, INPUT_DIM, OUTPUT_DIM, train2_y, \"Training\", \n",
    "                                         lra_loss, train2_uniform_x_samples, train2_nb_examples, \"train_curve_2\")\n",
    "\n",
    "lra_loss = regression_functions.evaluate_lra(linear_regressor_ann, test2_x, test2_nb_examples, INPUT_DIM, OUTPUT_DIM, test2_y, \"Test\")\n",
    "regression_functions.plot_lra_evaluation(linear_regressor_ann, test2_x, INPUT_DIM, OUTPUT_DIM, test2_y, \"Test\",\n",
    "                                         lra_loss, train2_uniform_x_samples, train2_nb_examples, \"test_curve_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_config = {2: 9e-3, 4: 9e-3, 8: 1e-2, 16: 9e-3}\n",
    "epoch_config = {2: 7500, 4: 6500, 8: 8000, 16: 30000}\n",
    "batchsize_config = {2: 3, 4: 3, 8: 2, 16: 2}\n",
    "activation_config = {2: \"sigmoid\", 4: \"sigmoid\", 8: \"sigmoid\", 16: \"sigmoid\"}\n",
    "loss_config = {2: \"mse\", 4: \"mse\", 8: \"mse\", 16: \"mse\"}\n",
    "momentum_config = {2: 0.4, 4: 0.4, 8: 0.5, 16: 0.3} # use 0 for no momentum\n",
    "stop_loss_config = {2: 0.28005, 4: 0.14305, 8: 0.05975, 16: 0.05915}\n",
    "plot_color = {2: \"red\", 4: \"cyan\", 8: \"magenta\", 16: \"black\"}\n",
    "\n",
    "trained_nets_2 = []\n",
    "\n",
    "for nb_of_hiddenunits in (2, 4, 8, 16):\n",
    "    np.random.seed(550)\n",
    "    learning_rate = lr_config[nb_of_hiddenunits]\n",
    "    nb_of_epochs = epoch_config[nb_of_hiddenunits]\n",
    "    batch_size = batchsize_config[nb_of_hiddenunits]\n",
    "\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation_config[nb_of_hiddenunits], \n",
    "                                loss_function=loss_config[nb_of_hiddenunits], \n",
    "                                use_momentum=True, momentum_factor=momentum_config[nb_of_hiddenunits]) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (1500 if nb_of_hiddenunits == 16 else 500) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), \n",
    "                     color=plot_color[nb_of_hiddenunits], linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/tla_{nb_of_hiddenunits}_2/tla_{epoch+1:05d}.png')\n",
    "            plt.close()\n",
    "        if tla_loss < stop_loss_config[nb_of_hiddenunits]:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), \n",
    "                     color=plot_color[nb_of_hiddenunits], linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2.png')\n",
    "            plt.close()\n",
    "            break\n",
    "    anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "    \n",
    "    frames = []\n",
    "    filenames = glob.glob(f'gif/tla_{nb_of_hiddenunits}_2/tla_*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    \n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "    trained_nets_2.append(two_layer_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 2\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 4\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 16\n",
    "anim_file = f'gif/tla_{nb_of_hiddenunits}_training2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_hidden_units = [2, 4, 8, 16]\n",
    "for i in range(4):\n",
    "    two_layer_ann = trained_nets_2[i]\n",
    "    tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "    tla_loss_std = np.std(two_layer_ann.loss(train2_y))\n",
    "    print(f\"Two layer ANN, {ann_hidden_units[i]} units, training set loss:{tla_loss:.4f}, std:{tla_loss_std:.4f}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.scatter(train2_x, train2_y)\n",
    "    tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), \n",
    "             color=plot_color[ann_hidden_units[i]], linewidth=3)\n",
    "    plt.title(f'Two layer ANN, {ann_hidden_units[i]} units, Training Set, Loss:{np.mean(tla_loss):.4f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.savefig(f'output/tla_{ann_hidden_units[i]}_train_curve_2.png')\n",
    "    plt.show()\n",
    "\n",
    "    tla_output = two_layer_ann.forward(test2_x.reshape((test2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    tla_loss = np.mean(two_layer_ann.loss(test2_y))\n",
    "    tla_loss_std = np.std(two_layer_ann.loss(test2_y))\n",
    "    print(f\"Two layer ANN, {ann_hidden_units[i]} units, test set loss:{tla_loss:.4f}, std:{tla_loss_std:.4f}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.scatter(test2_x, test2_y)\n",
    "    tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "    plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), \n",
    "             color=plot_color[ann_hidden_units[i]], linewidth=3)\n",
    "    plt.title(f'Two layer ANN, {ann_hidden_units[i]} units, Test Set, Loss:{np.mean(tla_loss):.4f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.savefig(f'output/tla_{ann_hidden_units[i]}_test_curve_2.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(train2_x, train2_y)\n",
    "\n",
    "two_layer_ann = trained_nets_2[0]\n",
    "tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), label='2 Units', \n",
    "             color=plot_color[2], linewidth=3)\n",
    "\n",
    "two_layer_ann = trained_nets_2[1]\n",
    "tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), label='4 Units', \n",
    "             color=plot_color[4], linewidth=3)\n",
    "\n",
    "two_layer_ann = trained_nets_2[2]\n",
    "tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), label='8 Units', \n",
    "             color=plot_color[8], linewidth=3)\n",
    "\n",
    "two_layer_ann = trained_nets_2[3]\n",
    "tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), label='16 Units', \n",
    "             color=plot_color[16], linewidth=3)\n",
    "\n",
    "leg = plt.legend(loc='lower left')\n",
    "\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(3)\n",
    "\n",
    "plt.title(f'Two layer ANNs with different number of hidden units')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('output/tla_all_curves_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "lr = {0: 1, 1: 1e-1, 2: 1e-2, 3: 1e-3, 4: 1e-4}\n",
    "nb_epoch = {0: 1, 1: 500, 2: 2500, 3: 25000, 4: 225000}\n",
    "batch_size = 229\n",
    "activation = \"sigmoid\"\n",
    "loss = \"mse\"\n",
    "momentum = 0.99\n",
    "stop_loss = 0.12\n",
    "plot_color = \"magenta\"\n",
    "\n",
    "for j in lr:\n",
    "    np.random.seed(550)\n",
    "    learning_rate = lr[j]\n",
    "    nb_of_epochs = nb_epoch[j]\n",
    "\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation, \n",
    "                                loss_function=loss, \n",
    "                                use_momentum=True, momentum_factor=momentum) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units, LR:{learning_rate}\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (1 if j == 0 else (500 if j < 3 else (10000 if j == 3 else 30000))) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "        if tla_loss < stop_loss:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color,\n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_d_{j}.png')\n",
    "            plt.show()\n",
    "            break\n",
    "        if epoch == nb_of_epochs - 1:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color, \n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_d_{j}.png')\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "learning_rate = 1e-2\n",
    "nb_of_epochs = 300000\n",
    "batch_size = 229\n",
    "activation = \"sigmoid\"\n",
    "loss = \"mse\"\n",
    "mf = {0: 0, 1: 0.99}\n",
    "stop_loss = 0.12\n",
    "plot_color = \"magenta\"\n",
    "\n",
    "for j in mf:\n",
    "    np.random.seed(550)\n",
    "    momentum = mf[j]\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation, \n",
    "                                loss_function=loss, \n",
    "                                use_momentum=True, momentum_factor=momentum) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units, MF:{momentum}\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (40000 if j == 0 else 500) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "        if tla_loss < stop_loss:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color,\n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_e_{j}.png')\n",
    "            plt.show()\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "learning_rates = {0: 8e-3, 1: 1.5e-2}\n",
    "nb_of_epochs_config = {0: 3500, 1: 110000} \n",
    "batch_sizes = {0: 1, 1: 229}\n",
    "activation = \"sigmoid\"\n",
    "loss = \"mse\"\n",
    "mf = {0: 0.1, 1: 0.2}\n",
    "stop_loss = 0.125\n",
    "plot_color = \"magenta\"\n",
    "\n",
    "for j in batch_sizes:\n",
    "    np.random.seed(550)\n",
    "    learning_rate = learning_rates[j]\n",
    "    batch_size = batch_sizes[j]\n",
    "    momentum = mf[j]\n",
    "    nb_of_epochs = nb_of_epochs_config[j]\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation, \n",
    "                                loss_function=loss, \n",
    "                                use_momentum=True, momentum_factor=momentum) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units, BS:{batch_size}\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (500 if j == 0 else 20000) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "        if tla_loss < stop_loss:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color,\n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_f_{j}.png')\n",
    "            plt.show()\n",
    "            break\n",
    "        if epoch == nb_of_epochs - 1:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color, \n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_f_{j}.png')\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
