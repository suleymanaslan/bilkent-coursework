{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import imageio\n",
    "import glob\n",
    "from IPython import display\n",
    "\n",
    "seaborn.set()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import utils\n",
    "import neural_net\n",
    "import regression_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_x, train1_y, train2_x, train2_y, test1_x, test1_y, test2_x, test2_y = utils.read_dataset()\n",
    "train1_x, train1_y, train2_x, train2_y, test1_x, test1_y, test2_x, test2_y = utils.normalize_dataset(train1_x, train1_y, train2_x, train2_y, test1_x, test1_y, test2_x, test2_y)\n",
    "\n",
    "utils.plot_dataset(train1_x, train1_y, train2_x, train2_y, test1_x, test1_y, test2_x, test2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_nb_examples, train2_nb_examples, test1_nb_examples, test2_nb_examples = utils.get_shape(train1_x, train2_x, test1_x, test2_x)\n",
    "train1_uniform_x_samples, train2_uniform_x_samples = utils.get_uniform_samples(train1_x, train1_nb_examples, train2_x, train2_nb_examples)\n",
    "\n",
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "np.random.seed(550)\n",
    "linear_regressor_ann = neural_net.LinearRegressorANN()\n",
    "lra_output = linear_regressor_ann.forward(train1_uniform_x_samples.reshape((train1_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "\n",
    "two_layer_ann = neural_net.TwoLayerANN(units=2)\n",
    "tla_output = two_layer_ann.forward(train1_uniform_x_samples.reshape(train1_nb_examples, INPUT_DIM, OUTPUT_DIM))\n",
    "\n",
    "utils.plot_output(train1_x, train1_y, train1_uniform_x_samples, lra_output.reshape((train1_nb_examples, 1)), 'Linear regressor ANN')\n",
    "utils.plot_output(train1_x, train1_y, train1_uniform_x_samples, tla_output.reshape((train1_nb_examples, 1)), 'Two layer ANN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear regressor ANN loss:{np.mean(linear_regressor_ann.loss(train1_y)):.2f}\")\n",
    "print(f\"Two layer ANN loss:{np.mean(two_layer_ann.loss(train1_y)):.2f}\")\n",
    "print(f\"Mean regressor loss:{np.mean((train1_y.reshape((train1_nb_examples, 1)) - np.full((train1_nb_examples, 1), np.mean(train1_y)))**2):.2f}\")\n",
    "\n",
    "linear_regressor_ann = neural_net.LinearRegressorANN()\n",
    "regression_functions.plot_lra_random_weight_losses(linear_regressor_ann, INPUT_DIM, OUTPUT_DIM, train1_x, train1_nb_examples, train1_y)\n",
    "\n",
    "two_layer_ann = neural_net.TwoLayerANN(units=2)\n",
    "regression_functions.plot_tla_random_weight_losses(two_layer_ann, INPUT_DIM, OUTPUT_DIM, 2, train1_x, train1_nb_examples, train1_y, randomize_first_layer=True)\n",
    "\n",
    "two_layer_ann = neural_net.TwoLayerANN(units=2)\n",
    "regression_functions.plot_tla_random_weight_losses(two_layer_ann, INPUT_DIM, OUTPUT_DIM, 2, train1_x, train1_nb_examples, train1_y, randomize_first_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "linear_regressor_ann = neural_net.LinearRegressorANN()\n",
    "regression_functions.train_lra([6e-2, 20, 30], linear_regressor_ann, train1_x, train1_y, INPUT_DIM, OUTPUT_DIM, train1_nb_examples, train1_uniform_x_samples)\n",
    "anim_file = \"gif/lra_training.gif\"\n",
    "utils.create_animation(anim_file, 'gif/lra_*.png')\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lra_loss = regression_functions.evaluate_lra(linear_regressor_ann, train1_x, train1_nb_examples, INPUT_DIM, OUTPUT_DIM, train1_y, \"Training\")\n",
    "regression_functions.plot_lra_evaluation(linear_regressor_ann, train1_x, INPUT_DIM, OUTPUT_DIM, train1_y, \"Training\", \n",
    "                                         lra_loss, train1_uniform_x_samples, train1_nb_examples, \"train_curve\")\n",
    "\n",
    "lra_loss = regression_functions.evaluate_lra(linear_regressor_ann, test1_x, test1_nb_examples, INPUT_DIM, OUTPUT_DIM, test1_y, \"Test\")\n",
    "regression_functions.plot_lra_evaluation(linear_regressor_ann, test1_x, INPUT_DIM, OUTPUT_DIM, test1_y, \"Test\",\n",
    "                                         lra_loss, train1_uniform_x_samples, train1_nb_examples, \"test_curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained_nets, anim_files = regression_functions.train_tla(1, train1_x, train1_y, INPUT_DIM, OUTPUT_DIM, train1_nb_examples, train1_uniform_x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regression_functions.evaluate_tla(trained_nets, train1_x, train1_y, test1_x, test1_y, train1_nb_examples, test1_nb_examples, train1_uniform_x_samples, INPUT_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_functions.plot_tla_curves(trained_nets, train1_x, train1_y, train1_uniform_x_samples, train1_nb_examples, INPUT_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "linear_regressor_ann = neural_net.LinearRegressorANN()\n",
    "regression_functions.train_lra([1.5e-1, 20, 229], linear_regressor_ann, train2_x, train2_y, INPUT_DIM, OUTPUT_DIM, train2_nb_examples, train2_uniform_x_samples, label=\"train_2\")\n",
    "anim_file = 'gif/lra_training_2.gif'\n",
    "utils.create_animation(anim_file, 'gif/lra_*.png')\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lra_loss = regression_functions.evaluate_lra(linear_regressor_ann, train2_x, train2_nb_examples, INPUT_DIM, OUTPUT_DIM, train2_y, \"Training\")\n",
    "regression_functions.plot_lra_evaluation(linear_regressor_ann, train2_x, INPUT_DIM, OUTPUT_DIM, train2_y, \"Training\", \n",
    "                                         lra_loss, train2_uniform_x_samples, train2_nb_examples, \"train_curve_2\")\n",
    "\n",
    "lra_loss = regression_functions.evaluate_lra(linear_regressor_ann, test2_x, test2_nb_examples, INPUT_DIM, OUTPUT_DIM, test2_y, \"Test\")\n",
    "regression_functions.plot_lra_evaluation(linear_regressor_ann, test2_x, INPUT_DIM, OUTPUT_DIM, test2_y, \"Test\",\n",
    "                                         lra_loss, train2_uniform_x_samples, train2_nb_examples, \"test_curve_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_nets_2, anim_files = regression_functions.train_tla(2, train2_x, train2_y, INPUT_DIM, OUTPUT_DIM, train2_nb_examples, train2_uniform_x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=anim_files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_functions.evaluate_tla(trained_nets_2, train2_x, train2_y, test2_x, test2_y, train2_nb_examples, test2_nb_examples, train2_uniform_x_samples, INPUT_DIM, OUTPUT_DIM, \"_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_functions.plot_tla_curves(trained_nets_2, train2_x, train2_y, train2_uniform_x_samples, train2_nb_examples, INPUT_DIM, OUTPUT_DIM, 'lower left', \"_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "lr = {0: 1, 1: 1e-1, 2: 1e-2, 3: 1e-3, 4: 1e-4}\n",
    "nb_epoch = {0: 1, 1: 500, 2: 2500, 3: 25000, 4: 225000}\n",
    "batch_size = 229\n",
    "activation = \"sigmoid\"\n",
    "loss = \"mse\"\n",
    "momentum = 0.99\n",
    "stop_loss = 0.12\n",
    "plot_color = \"magenta\"\n",
    "\n",
    "for j in lr:\n",
    "    np.random.seed(550)\n",
    "    learning_rate = lr[j]\n",
    "    nb_of_epochs = nb_epoch[j]\n",
    "\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation, \n",
    "                                loss_function=loss, \n",
    "                                use_momentum=True, momentum_factor=momentum) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units, LR:{learning_rate}\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (1 if j == 0 else (500 if j < 3 else (10000 if j == 3 else 30000))) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "        if tla_loss < stop_loss:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color,\n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_d_{j}.png')\n",
    "            plt.show()\n",
    "            break\n",
    "        if epoch == nb_of_epochs - 1:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color, \n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_d_{j}.png')\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "learning_rate = 1e-2\n",
    "nb_of_epochs = 300000\n",
    "batch_size = 229\n",
    "activation = \"sigmoid\"\n",
    "loss = \"mse\"\n",
    "mf = {0: 0, 1: 0.99}\n",
    "stop_loss = 0.12\n",
    "plot_color = \"magenta\"\n",
    "\n",
    "for j in mf:\n",
    "    np.random.seed(550)\n",
    "    momentum = mf[j]\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation, \n",
    "                                loss_function=loss, \n",
    "                                use_momentum=True, momentum_factor=momentum) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units, MF:{momentum}\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (40000 if j == 0 else 500) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "        if tla_loss < stop_loss:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color,\n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_e_{j}.png')\n",
    "            plt.show()\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_hiddenunits = 8\n",
    "learning_rates = {0: 8e-3, 1: 1.5e-2}\n",
    "nb_of_epochs_config = {0: 3500, 1: 110000} \n",
    "batch_sizes = {0: 1, 1: 229}\n",
    "activation = \"sigmoid\"\n",
    "loss = \"mse\"\n",
    "mf = {0: 0.1, 1: 0.2}\n",
    "stop_loss = 0.125\n",
    "plot_color = \"magenta\"\n",
    "\n",
    "for j in batch_sizes:\n",
    "    np.random.seed(550)\n",
    "    learning_rate = learning_rates[j]\n",
    "    batch_size = batch_sizes[j]\n",
    "    momentum = mf[j]\n",
    "    nb_of_epochs = nb_of_epochs_config[j]\n",
    "    two_layer_ann = TwoLayerANN(nb_of_hiddenunits, \n",
    "                                activation_function=activation, \n",
    "                                loss_function=loss, \n",
    "                                use_momentum=True, momentum_factor=momentum) # reset network\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('w')\n",
    "    print(f\"Training two layer ANN with {nb_of_hiddenunits} units, BS:{batch_size}\")\n",
    "    for epoch in range(nb_of_epochs):\n",
    "        for i in range(train2_nb_examples//batch_size):\n",
    "            two_layer_ann.forward(train2_x[i*batch_size:i*batch_size+batch_size].reshape((batch_size, INPUT_DIM, OUTPUT_DIM)))\n",
    "            two_layer_ann.loss(train2_y[i*batch_size:i*batch_size+batch_size])\n",
    "            two_layer_ann.backward(learning_rate)\n",
    "        tla_output = two_layer_ann.forward(train2_x.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "        tla_loss = np.mean(two_layer_ann.loss(train2_y))\n",
    "        if epoch == 0 or (epoch+1) % (500 if j == 0 else 20000) == 0:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "        if tla_loss < stop_loss:\n",
    "            print(f\"Stopped training, Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color,\n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_f_{j}.png')\n",
    "            plt.show()\n",
    "            break\n",
    "        if epoch == nb_of_epochs - 1:\n",
    "            print(f\"Epoch:{epoch+1}, Two layer ANN loss:{tla_loss:.4f}\")\n",
    "            plt.scatter(train2_x, train2_y)\n",
    "            tla_output = two_layer_ann.forward(train2_uniform_x_samples.reshape((train2_nb_examples, INPUT_DIM, OUTPUT_DIM)))\n",
    "            plt.plot(train2_uniform_x_samples, tla_output.reshape((train2_nb_examples, 1)), plot_color, \n",
    "                     linewidth=3)\n",
    "            plt.title(f'Two layer ANN ({nb_of_hiddenunits} units), Epoch:{epoch+1}, Training Set, Loss:{tla_loss:.4f}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'output/tla2_{nb_of_hiddenunits}_train2_f_{j}.png')\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
