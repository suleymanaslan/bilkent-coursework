{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import imageio\n",
    "import glob\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "import time\n",
    "\n",
    "seaborn.set()\n",
    "np.set_printoptions(suppress=True)\n",
    "hsv_colors = cm.get_cmap('hsv')\n",
    "\n",
    "def read_datafile(file):\n",
    "    with open(file, 'r') as datafile:\n",
    "        contents = datafile.read()\n",
    "        data = np.array([line.split('\\t') for line in contents.split('\\n')], dtype=np.float64)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['firebrick','darkorange','darkgoldenrod','forestgreen','dodgerblue','blueviolet','magenta', 'black']\n",
    "\n",
    "def normalize_data(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        data[:,i] = (data[:,i] - np.mean(data[:,i])) / np.std(data[:,i])\n",
    "    return data\n",
    "\n",
    "dataset1 = read_datafile('data/dataset1')\n",
    "dataset2 = read_datafile('data/dataset2')\n",
    "dataset3 = read_datafile('data/dataset3')\n",
    "print(f\"dataset1:{dataset1.shape}, dataset2:{dataset2.shape}, dataset3:{dataset3.shape}\")\n",
    "\n",
    "dataset1 = normalize_data(dataset1)\n",
    "dataset2 = normalize_data(dataset2)\n",
    "dataset3 = normalize_data(dataset3)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(dataset1[:,0], dataset1[:,1], c=color_list[-1])\n",
    "plt.title('dataset1')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(dataset2[:,0], dataset2[:,1], c=color_list[-1])\n",
    "plt.title('dataset2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('w')\n",
    "plt.scatter(dataset3[:,0], dataset3[:,1], c=color_list[-1])\n",
    "plt.title('dataset3')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centroids(data):\n",
    "    cluster_centroids = np.concatenate((np.random.uniform(low=data[:,0].min(), high=data[:,0].max(), size=K).reshape(K, 1),\n",
    "                                        np.random.uniform(low=data[:,1].min(), high=data[:,1].max(), size=K).reshape(K, 1)),\n",
    "                                       axis=1)\n",
    "    return cluster_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(data, cluster_centroids, data_clusters=None):\n",
    "    cluster_ids = np.arange(K)\n",
    "    nb_of_points = data.shape[0]\n",
    "    min_distances = np.full(nb_of_points, np.inf)\n",
    "    if data_clusters is None:\n",
    "        data_clusters = np.zeros(nb_of_points)\n",
    "    prev_data_clusters = np.copy(data_clusters)\n",
    "    for i in range(K):\n",
    "        cur_distances = np.square(data - cluster_centroids[i]).sum(axis=1)\n",
    "        if len(data_clusters[cur_distances < min_distances]) > 0:\n",
    "            data_clusters[cur_distances < min_distances] = i\n",
    "        min_distances = np.minimum(cur_distances, min_distances)\n",
    "    return data_clusters, np.array_equal(data_clusters, prev_data_clusters), min_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids(data, data_clusters, cluster_centroids):\n",
    "    for i in range(K):\n",
    "        if len(data[data_clusters == i]) > 0:\n",
    "            cluster_centroids[i,:] = data[data_clusters == i].mean(axis=0)\n",
    "    return cluster_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, dataname=\"data\", create_anim_file=False, print_output=True):\n",
    "    kmeans_iterations = 0\n",
    "    \n",
    "    if create_anim_file:\n",
    "        anim_iter = 0\n",
    "        plt.scatter(data[:,0], data[:,1], c=color_list[-1])\n",
    "        plt.title(f'{dataname}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.savefig(f'gif/kmeans/{dataname}/{anim_iter:04d}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    cluster_centroids = initialize_centroids(data)\n",
    "    \n",
    "    if create_anim_file:\n",
    "        anim_iter += 1\n",
    "        plt.scatter(data[:,0], data[:,1], c=color_list[-1])\n",
    "        for i in range(K):\n",
    "            plt.scatter(cluster_centroids[:,0][i], cluster_centroids[:,1][i], s=250, marker=\"X\", c=color_list[i], linewidth=3, \n",
    "                        edgecolors='black')\n",
    "        plt.title(f'{dataname}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.savefig(f'gif/kmeans/{dataname}/{anim_iter:04d}.png')\n",
    "        plt.close()\n",
    "\n",
    "    data_clusters, done, min_distances = assign_clusters(data, cluster_centroids)\n",
    "    kmeans_iterations += 1\n",
    "    \n",
    "    if create_anim_file:\n",
    "        anim_iter += 1\n",
    "        for i in range(K):\n",
    "            plt.scatter(data[data_clusters == i][:,0], data[data_clusters == i][:,1], c=color_list[i])\n",
    "            plt.scatter(cluster_centroids[:,0][i], cluster_centroids[:,1][i], s=250, marker=\"X\", c=color_list[i], linewidth=3, \n",
    "                        edgecolors='black')\n",
    "        plt.title(f'{dataname}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.savefig(f'gif/kmeans/{dataname}/{anim_iter:04d}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    while not done:\n",
    "        cluster_centroids = update_centroids(data, data_clusters, cluster_centroids)\n",
    "        \n",
    "        if create_anim_file:\n",
    "            anim_iter += 1\n",
    "            for i in range(K):\n",
    "                plt.scatter(data[data_clusters == i][:,0], data[data_clusters == i][:,1], c=color_list[i])\n",
    "                plt.scatter(cluster_centroids[:,0][i], cluster_centroids[:,1][i], s=250, marker=\"X\", c=color_list[i],\n",
    "                            linewidth=3, edgecolors='black')\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/kmeans/{dataname}/{anim_iter:04d}.png')\n",
    "            plt.close()\n",
    "        \n",
    "        data_clusters, done, min_distances = assign_clusters(data, cluster_centroids, data_clusters=data_clusters)\n",
    "        kmeans_iterations += 1\n",
    "\n",
    "        if create_anim_file:\n",
    "            anim_iter += 1\n",
    "            for i in range(K):\n",
    "                plt.scatter(data[data_clusters == i][:,0], data[data_clusters == i][:,1], c=color_list[i])\n",
    "                plt.scatter(cluster_centroids[:,0][i], cluster_centroids[:,1][i], s=250, marker=\"X\", c=color_list[i], \n",
    "                            linewidth=3, edgecolors='black')\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/kmeans/{dataname}/{anim_iter:04d}.png')\n",
    "            plt.close()\n",
    "\n",
    "    if print_output:\n",
    "        print(f\"Kmeans for {dataname} finished with {kmeans_iterations} iterations\")\n",
    "        print(f\"Sum of squared errors for {dataname} (normalized) with kmeans:{min_distances.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anim_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "K = 7\n",
    "kmeans(dataset1, \"dataset1\", create_anim_file=create_anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "K = 3\n",
    "kmeans(dataset2, \"dataset2\", create_anim_file=create_anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "K = 2\n",
    "kmeans(dataset3, \"dataset3\", create_anim_file=create_anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/kmeans_dataset1.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/kmeans/dataset1/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/kmeans_dataset1.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/kmeans_dataset2.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/kmeans/dataset2/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/kmeans_dataset2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/kmeans_dataset3.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/kmeans/dataset3/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/kmeans_dataset3.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "K = 7\n",
    "%timeit kmeans(dataset1, \"dataset1\", create_anim_file=False, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "K = 3\n",
    "%timeit kmeans(dataset2, \"dataset2\", create_anim_file=False, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(550)\n",
    "K = 2\n",
    "%timeit kmeans(dataset3, \"dataset3\", create_anim_file=False, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(dataset):\n",
    "    sm = np.array([np.square(dataset - dataset[i]).sum(axis=1) for i in range(len(dataset))])\n",
    "    sm[np.identity(len(sm)) == 1] = np.inf\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(distance_matrix, clusters, len_dm):\n",
    "    argmin_dm = np.argmin(distance_matrix)\n",
    "    min_i, min_j = argmin_dm // len_dm, argmin_dm % len_dm\n",
    "    cluster_min_i, cluster_min_j = clusters[min_i], clusters[min_j]\n",
    "    if not cluster_min_i == cluster_min_j:\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:, cluster_eq_j] = np.inf\n",
    "        distance_matrix[cluster_eq_i] = temp_dm    \n",
    "        temp_dm = distance_matrix[cluster_eq_j]\n",
    "        temp_dm[:, cluster_eq_i] = np.inf\n",
    "        distance_matrix[cluster_eq_j] = temp_dm\n",
    "        clusters[cluster_eq_j] = cluster_min_i\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "    return distance_matrix, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_linkage(dataset, nb_of_clusters, dataname=\"data\", create_anim_file=False, plot_every_iter=15, print_output=True):\n",
    "    start_time = time.time()\n",
    "    dm = compute_distance_matrix(dataset)\n",
    "    clusters = np.arange(len(dataset))\n",
    "    len_dm = len(dm)\n",
    "    \n",
    "    for i in range(len(dataset) - nb_of_clusters):\n",
    "        dm, clusters = merge_clusters(dm, clusters, len_dm)\n",
    "        \n",
    "        unique_clusters = np.unique(clusters)\n",
    "        if create_anim_file and (i % plot_every_iter == 0 or len(unique_clusters) < 8):\n",
    "            if len(unique_clusters) < 8:\n",
    "                color_arr = [color_list[np.where(unique_clusters == cluster)[0][0]] for cluster in clusters]\n",
    "            else:\n",
    "                color_arr = [hsv_colors(cluster/len(dataset)) for cluster in clusters]\n",
    "            plt.scatter(dataset[:,0], dataset[:,1], c=color_arr)\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/singlelinkage/{dataname}/{i:04d}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    squared_errors = 0\n",
    "    for i in unique_clusters:\n",
    "        cluster_centroid = dataset[clusters == i].mean(axis=0)\n",
    "        squared_error = np.square(dataset[clusters == i] - cluster_centroid).sum()\n",
    "        squared_errors += squared_error\n",
    "        \n",
    "    end_time = time.time()\n",
    "    if print_output:\n",
    "        print(f\"Sum of squared errors for {dataname} (normalized) with single-linkage:{squared_errors:.4f}\")\n",
    "        print(f\"Single-linkage for {dataname} took :{end_time - start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anim_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_linkage(dataset1, 7, dataname=\"dataset1\", create_anim_file=create_anim_file, plot_every_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_linkage(dataset2, 3, dataname=\"dataset2\", create_anim_file=create_anim_file, plot_every_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_linkage(dataset3, 2, dataname=\"dataset3\", create_anim_file=create_anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/singlelinkage_dataset1.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/singlelinkage/dataset1/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/singlelinkage_dataset1.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/singlelinkage_dataset2.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/singlelinkage/dataset2/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/singlelinkage_dataset2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/singlelinkage_dataset3.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/singlelinkage/dataset3/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/singlelinkage_dataset3.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(dataset):\n",
    "    sm = np.array([np.square(dataset - dataset[i]).sum(axis=1) for i in range(len(dataset))])\n",
    "    sm[np.identity(len(sm)) == 1] = np.nan\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(distance_matrix, clusters, len_dm):\n",
    "    nanargmin_dm = np.nanargmin(distance_matrix)\n",
    "    min_i, min_j = nanargmin_dm // len_dm, nanargmin_dm % len_dm\n",
    "    cluster_min_i, cluster_min_j = clusters[min_i], clusters[min_j]\n",
    "    if not cluster_min_i == cluster_min_j:\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:, cluster_eq_j] = np.nan\n",
    "        distance_matrix[cluster_eq_i] = temp_dm    \n",
    "        temp_dm = distance_matrix[cluster_eq_j]\n",
    "        temp_dm[:, cluster_eq_i] = np.nan\n",
    "        distance_matrix[cluster_eq_j] = temp_dm\n",
    "        clusters[cluster_eq_j] = cluster_min_i\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:] = np.max(temp_dm, axis=0)\n",
    "        distance_matrix[cluster_eq_i] = temp_dm\n",
    "        temp_dm = distance_matrix[:,cluster_eq_i]\n",
    "        temp_dm[:] = np.expand_dims(np.max(temp_dm, axis=1), axis=1)\n",
    "        distance_matrix[:,cluster_eq_i] = temp_dm\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "    return distance_matrix, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_linkage(dataset, nb_of_clusters, dataname=\"data\", create_anim_file=False, plot_every_iter=15, print_output=True):\n",
    "    start_time = time.time()\n",
    "    dm = compute_distance_matrix(dataset)\n",
    "    clusters = np.arange(len(dataset))\n",
    "    len_dm = len(dm)\n",
    "    \n",
    "    for i in range(len(dataset) - nb_of_clusters):\n",
    "        dm, clusters = merge_clusters(dm, clusters, len_dm)\n",
    "        \n",
    "        unique_clusters = np.unique(clusters)\n",
    "        if create_anim_file and (i % plot_every_iter == 0 or len(unique_clusters) < 8):\n",
    "            if len(unique_clusters) < 8:\n",
    "                color_arr = [color_list[np.where(unique_clusters == cluster)[0][0]] for cluster in clusters]\n",
    "            else:\n",
    "                color_arr = [hsv_colors(cluster/len(dataset)) for cluster in clusters]\n",
    "            plt.scatter(dataset[:,0], dataset[:,1], c=color_arr)\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/completelinkage/{dataname}/{i:04d}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    squared_errors = 0\n",
    "    for i in unique_clusters:\n",
    "        cluster_centroid = dataset[clusters == i].mean(axis=0)\n",
    "        squared_error = np.square(dataset[clusters == i] - cluster_centroid).sum()\n",
    "        squared_errors += squared_error\n",
    "        \n",
    "    end_time = time.time()\n",
    "    if print_output:\n",
    "        print(f\"Sum of squared errors for {dataname} (normalized) with complete-linkage:{squared_errors:.4f}\")\n",
    "        print(f\"Complete-linkage for {dataname} took :{end_time - start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anim_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_linkage(dataset1, 7, dataname=\"dataset1\", create_anim_file=create_anim_file, plot_every_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_linkage(dataset2, 3, dataname=\"dataset2\", create_anim_file=create_anim_file, plot_every_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_linkage(dataset3, 2, dataname=\"dataset3\", create_anim_file=create_anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/completelinkage_dataset1.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/completelinkage/dataset1/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/completelinkage_dataset1.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/completelinkage_dataset2.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/completelinkage/dataset2/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/completelinkage_dataset2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/completelinkage_dataset3.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/completelinkage/dataset3/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/completelinkage_dataset3.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(dataset):\n",
    "    sm = np.array([np.square(dataset - dataset[i]).sum(axis=1) for i in range(len(dataset))])\n",
    "    sm[np.identity(len(sm)) == 1] = np.nan\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(distance_matrix, clusters, len_dm):\n",
    "    nanargmin_dm = np.nanargmin(distance_matrix)\n",
    "    min_i, min_j = nanargmin_dm // len_dm, nanargmin_dm % len_dm\n",
    "    cluster_min_i, cluster_min_j = clusters[min_i], clusters[min_j]\n",
    "    if not cluster_min_i == cluster_min_j:\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:, cluster_eq_j] = np.nan\n",
    "        distance_matrix[cluster_eq_i] = temp_dm    \n",
    "        temp_dm = distance_matrix[cluster_eq_j]\n",
    "        temp_dm[:, cluster_eq_i] = np.nan\n",
    "        distance_matrix[cluster_eq_j] = temp_dm\n",
    "        clusters[cluster_eq_j] = cluster_min_i\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:] = np.mean(temp_dm, axis=0)\n",
    "        distance_matrix[cluster_eq_i] = temp_dm\n",
    "        temp_dm = distance_matrix[:,cluster_eq_i]\n",
    "        temp_dm[:] = np.expand_dims(np.mean(temp_dm, axis=1), axis=1)\n",
    "        distance_matrix[:,cluster_eq_i] = temp_dm\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "    return distance_matrix, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_average(dataset, nb_of_clusters, dataname=\"data\", create_anim_file=False, plot_every_iter=15, print_output=True):\n",
    "    start_time = time.time()\n",
    "    dm = compute_distance_matrix(dataset)\n",
    "    clusters = np.arange(len(dataset))\n",
    "    len_dm = len(dm)\n",
    "    \n",
    "    for i in range(len(dataset) - nb_of_clusters):\n",
    "        dm, clusters = merge_clusters(dm, clusters, len_dm)\n",
    "        \n",
    "        unique_clusters = np.unique(clusters)\n",
    "        if create_anim_file and (i % plot_every_iter == 0 or len(unique_clusters) < 8):\n",
    "            if len(unique_clusters) < 8:\n",
    "                color_arr = [color_list[np.where(unique_clusters == cluster)[0][0]] for cluster in clusters]\n",
    "            else:\n",
    "                color_arr = [hsv_colors(cluster/len(dataset)) for cluster in clusters]\n",
    "            plt.scatter(dataset[:,0], dataset[:,1], c=color_arr)\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/groupaverage/{dataname}/{i:04d}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    squared_errors = 0\n",
    "    for i in unique_clusters:\n",
    "        cluster_centroid = dataset[clusters == i].mean(axis=0)\n",
    "        squared_error = np.square(dataset[clusters == i] - cluster_centroid).sum()\n",
    "        squared_errors += squared_error\n",
    "        \n",
    "    end_time = time.time()\n",
    "    if print_output:\n",
    "        print(f\"Sum of squared errors for {dataname} (normalized) with group average:{squared_errors:.4f}\")\n",
    "        print(f\"Group average for {dataname} took :{end_time - start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anim_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_average(dataset1, 7, dataname=\"dataset1\", create_anim_file=create_anim_file, plot_every_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_average(dataset2, 3, dataname=\"dataset2\", create_anim_file=create_anim_file, plot_every_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_average(dataset3, 2, dataname=\"dataset3\", create_anim_file=create_anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/groupaverage_dataset1.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/groupaverage/dataset1/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/groupaverage_dataset1.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/groupaverage_dataset2.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/groupaverage/dataset2/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/groupaverage_dataset2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/groupaverage_dataset3.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/groupaverage/dataset3/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/groupaverage_dataset3.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(dataset, min_pts, eps, create_anim_file=False, dataname=\"data\"):\n",
    "    start_time = time.time()\n",
    "    eps_c = eps * eps\n",
    "\n",
    "    core_point_list = []\n",
    "    for point_index, cur_point in enumerate(dataset):\n",
    "        points_within_eps = 0\n",
    "        for other_point_index, other_point in enumerate(dataset):\n",
    "            if point_index == other_point_index:\n",
    "                continue\n",
    "            if np.square(cur_point - other_point).sum() < eps_c:\n",
    "                points_within_eps += 1\n",
    "            if points_within_eps >= min_pts:\n",
    "                core_point_list.append(point_index)\n",
    "                break\n",
    "\n",
    "    core_points = dataset[core_point_list]\n",
    "    non_core_points = np.delete(dataset, core_point_list, axis=0)\n",
    "\n",
    "    border_point_list = []\n",
    "    for non_core_point_index, non_core_point in enumerate(non_core_points):\n",
    "        for core_point in core_points:\n",
    "            if np.square(non_core_point - core_point).sum() < eps_c:\n",
    "                border_point_list.append(non_core_point_index)\n",
    "                break\n",
    "\n",
    "    border_points = non_core_points[border_point_list]\n",
    "    noise_points = np.delete(non_core_points, border_point_list, axis=0)\n",
    "\n",
    "    cur_cluster = -1\n",
    "    clusters = np.full(len(dataset), cur_cluster)\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    \n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "         \n",
    "        if create_anim_file and len(unique_clusters) <= 8:\n",
    "            color_arr = [color_list[cluster] for cluster in clusters]\n",
    "            plt.scatter(dataset[:,0], dataset[:,1], c=color_arr)\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/dbscan/{dataname}/{i:04d}.png')\n",
    "            plt.close()\n",
    "        \n",
    "        \n",
    "        occurence = np.nonzero(clusters[core_point_list] == -1)\n",
    "        if len(occurence[0]) > 0:\n",
    "            core_point_occurence = occurence[0][0]\n",
    "            cur_cluster += 1\n",
    "            clusters[core_point_list[core_point_occurence]] = cur_cluster\n",
    "\n",
    "            core_point = dataset[core_point_list[core_point_occurence]]\n",
    "            other_core_indices = []\n",
    "            for other_point_index, other_point in enumerate(dataset):\n",
    "                if clusters[other_point_index] == -1 and np.square(core_point - other_point).sum() < eps_c:\n",
    "                    clusters[other_point_index] = clusters[core_point_list[core_point_occurence]]\n",
    "                    if other_point_index in core_point_list:\n",
    "                        other_core_indices.append(other_point_index)\n",
    "            for other_core_index in other_core_indices:\n",
    "                new_core_point = dataset[other_core_index]\n",
    "                for other_point_index, other_point in enumerate(dataset):\n",
    "                    if clusters[other_point_index] == -1 and np.square(new_core_point - other_point).sum() < eps_c:\n",
    "                        clusters[other_point_index] = clusters[other_core_index]\n",
    "                        if other_point_index in core_point_list:\n",
    "                            other_core_indices.append(other_point_index)\n",
    "        else:\n",
    "            done = True\n",
    "        \n",
    "        i += 1\n",
    "            \n",
    "    unique_clusters = np.unique(clusters)\n",
    "    squared_errors = 0\n",
    "    for i in unique_clusters:\n",
    "        if i == -1:\n",
    "            continue\n",
    "        cluster_centroid = dataset[clusters == i].mean(axis=0)\n",
    "        squared_error = np.square(dataset[clusters == i] - cluster_centroid).sum()\n",
    "        squared_errors += squared_error\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Sum of squared errors for {dataname} (normalized) with DBSCAN:{squared_errors:.4f}\")\n",
    "    print(f\"DBSCAN for {dataname} took :{end_time - start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anim_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan(dataset1, min_pts=230, eps=0.795, create_anim_file=create_anim_file, dataname=\"dataset1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan(dataset2, min_pts=30, eps=0.4, create_anim_file=create_anim_file, dataname=\"dataset2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan(dataset3, min_pts=45, eps=0.927, create_anim_file=create_anim_file, dataname=\"dataset3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/dbscan_dataset1.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/dbscan/dataset1/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(3):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=2)\n",
    "    \n",
    "anim_file = 'gif/dbscan_dataset1.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/dbscan_dataset2.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/dbscan/dataset2/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(3):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=2)\n",
    "    \n",
    "anim_file = 'gif/dbscan_dataset2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/dbscan_dataset3.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/dbscan/dataset3/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(3):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=2)\n",
    "    \n",
    "anim_file = 'gif/dbscan_dataset3.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
