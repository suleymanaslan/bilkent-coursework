{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import imageio\n",
    "import glob\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "import time\n",
    "\n",
    "seaborn.set()\n",
    "np.set_printoptions(suppress=True)\n",
    "hsv_colors = cm.get_cmap('hsv')\n",
    "\n",
    "import utils\n",
    "import clustering_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1, dataset2, dataset3 = utils.read_dataset2()\n",
    "dataset1, dataset2, dataset3 = utils.normalize_dataset2(dataset1, dataset2, dataset3)\n",
    "\n",
    "color_list = ['firebrick','darkorange','darkgoldenrod','forestgreen','dodgerblue','blueviolet','magenta', 'black']\n",
    "utils.plot_dataset2(dataset1, dataset2, dataset3, color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_functions.kmeans(dataset1, k=7, dataname=\"dataset1\", create_anim_file=True, color_list=color_list)\n",
    "display.Image(filename='gif/kmeans_dataset1.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_functions.kmeans(dataset2, k=3, dataname=\"dataset2\", create_anim_file=True, color_list=color_list)\n",
    "display.Image(filename='gif/kmeans_dataset2.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_functions.kmeans(dataset3, k=2, dataname=\"dataset3\", create_anim_file=True, color_list=color_list)\n",
    "display.Image(filename='gif/kmeans_dataset3.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit clustering_functions.kmeans(dataset1, k=7, dataname=\"dataset1\", create_anim_file=False, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit clustering_functions.kmeans(dataset2, k=3, dataname=\"dataset2\", create_anim_file=False, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit clustering_functions.kmeans(dataset3, k=2, dataname=\"dataset3\", create_anim_file=False, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(dataset):\n",
    "    sm = np.array([np.square(dataset - dataset[i]).sum(axis=1) for i in range(len(dataset))])\n",
    "    sm[np.identity(len(sm)) == 1] = np.inf\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(distance_matrix, clusters, len_dm):\n",
    "    argmin_dm = np.argmin(distance_matrix)\n",
    "    min_i, min_j = argmin_dm // len_dm, argmin_dm % len_dm\n",
    "    cluster_min_i, cluster_min_j = clusters[min_i], clusters[min_j]\n",
    "    if not cluster_min_i == cluster_min_j:\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:, cluster_eq_j] = np.inf\n",
    "        distance_matrix[cluster_eq_i] = temp_dm    \n",
    "        temp_dm = distance_matrix[cluster_eq_j]\n",
    "        temp_dm[:, cluster_eq_i] = np.inf\n",
    "        distance_matrix[cluster_eq_j] = temp_dm\n",
    "        clusters[cluster_eq_j] = cluster_min_i\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "    return distance_matrix, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_linkage(dataset, nb_of_clusters, dataname=\"data\", create_anim_file=False, plot_every_iter=15, print_output=True):\n",
    "    start_time = time.time()\n",
    "    dm = compute_distance_matrix(dataset)\n",
    "    clusters = np.arange(len(dataset))\n",
    "    len_dm = len(dm)\n",
    "    \n",
    "    for i in range(len(dataset) - nb_of_clusters):\n",
    "        dm, clusters = merge_clusters(dm, clusters, len_dm)\n",
    "        \n",
    "        unique_clusters = np.unique(clusters)\n",
    "        if create_anim_file and (i % plot_every_iter == 0 or len(unique_clusters) < 8):\n",
    "            if len(unique_clusters) < 8:\n",
    "                color_arr = [color_list[np.where(unique_clusters == cluster)[0][0]] for cluster in clusters]\n",
    "            else:\n",
    "                color_arr = [hsv_colors(cluster/len(dataset)) for cluster in clusters]\n",
    "            plt.scatter(dataset[:,0], dataset[:,1], c=color_arr)\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/singlelinkage/{dataname}/{i:04d}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    squared_errors = 0\n",
    "    for i in unique_clusters:\n",
    "        cluster_centroid = dataset[clusters == i].mean(axis=0)\n",
    "        squared_error = np.square(dataset[clusters == i] - cluster_centroid).sum()\n",
    "        squared_errors += squared_error\n",
    "        \n",
    "    end_time = time.time()\n",
    "    if print_output:\n",
    "        print(f\"Sum of squared errors for {dataname} (normalized) with single-linkage:{squared_errors:.4f}\")\n",
    "        print(f\"Single-linkage for {dataname} took :{end_time - start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anim_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_linkage(dataset1, 7, dataname=\"dataset1\", create_anim_file=create_anim_file, plot_every_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_linkage(dataset2, 3, dataname=\"dataset2\", create_anim_file=create_anim_file, plot_every_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_linkage(dataset3, 2, dataname=\"dataset3\", create_anim_file=create_anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/singlelinkage_dataset1.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/singlelinkage/dataset1/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/singlelinkage_dataset1.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/singlelinkage_dataset2.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/singlelinkage/dataset2/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/singlelinkage_dataset2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/singlelinkage_dataset3.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/singlelinkage/dataset3/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/singlelinkage_dataset3.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(dataset):\n",
    "    sm = np.array([np.square(dataset - dataset[i]).sum(axis=1) for i in range(len(dataset))])\n",
    "    sm[np.identity(len(sm)) == 1] = np.nan\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(distance_matrix, clusters, len_dm):\n",
    "    nanargmin_dm = np.nanargmin(distance_matrix)\n",
    "    min_i, min_j = nanargmin_dm // len_dm, nanargmin_dm % len_dm\n",
    "    cluster_min_i, cluster_min_j = clusters[min_i], clusters[min_j]\n",
    "    if not cluster_min_i == cluster_min_j:\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:, cluster_eq_j] = np.nan\n",
    "        distance_matrix[cluster_eq_i] = temp_dm    \n",
    "        temp_dm = distance_matrix[cluster_eq_j]\n",
    "        temp_dm[:, cluster_eq_i] = np.nan\n",
    "        distance_matrix[cluster_eq_j] = temp_dm\n",
    "        clusters[cluster_eq_j] = cluster_min_i\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:] = np.max(temp_dm, axis=0)\n",
    "        distance_matrix[cluster_eq_i] = temp_dm\n",
    "        temp_dm = distance_matrix[:,cluster_eq_i]\n",
    "        temp_dm[:] = np.expand_dims(np.max(temp_dm, axis=1), axis=1)\n",
    "        distance_matrix[:,cluster_eq_i] = temp_dm\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "    return distance_matrix, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_linkage(dataset, nb_of_clusters, dataname=\"data\", create_anim_file=False, plot_every_iter=15, print_output=True):\n",
    "    start_time = time.time()\n",
    "    dm = compute_distance_matrix(dataset)\n",
    "    clusters = np.arange(len(dataset))\n",
    "    len_dm = len(dm)\n",
    "    \n",
    "    for i in range(len(dataset) - nb_of_clusters):\n",
    "        dm, clusters = merge_clusters(dm, clusters, len_dm)\n",
    "        \n",
    "        unique_clusters = np.unique(clusters)\n",
    "        if create_anim_file and (i % plot_every_iter == 0 or len(unique_clusters) < 8):\n",
    "            if len(unique_clusters) < 8:\n",
    "                color_arr = [color_list[np.where(unique_clusters == cluster)[0][0]] for cluster in clusters]\n",
    "            else:\n",
    "                color_arr = [hsv_colors(cluster/len(dataset)) for cluster in clusters]\n",
    "            plt.scatter(dataset[:,0], dataset[:,1], c=color_arr)\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/completelinkage/{dataname}/{i:04d}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    squared_errors = 0\n",
    "    for i in unique_clusters:\n",
    "        cluster_centroid = dataset[clusters == i].mean(axis=0)\n",
    "        squared_error = np.square(dataset[clusters == i] - cluster_centroid).sum()\n",
    "        squared_errors += squared_error\n",
    "        \n",
    "    end_time = time.time()\n",
    "    if print_output:\n",
    "        print(f\"Sum of squared errors for {dataname} (normalized) with complete-linkage:{squared_errors:.4f}\")\n",
    "        print(f\"Complete-linkage for {dataname} took :{end_time - start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anim_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_linkage(dataset1, 7, dataname=\"dataset1\", create_anim_file=create_anim_file, plot_every_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_linkage(dataset2, 3, dataname=\"dataset2\", create_anim_file=create_anim_file, plot_every_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_linkage(dataset3, 2, dataname=\"dataset3\", create_anim_file=create_anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/completelinkage_dataset1.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/completelinkage/dataset1/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/completelinkage_dataset1.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/completelinkage_dataset2.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/completelinkage/dataset2/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/completelinkage_dataset2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/completelinkage_dataset3.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/completelinkage/dataset3/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/completelinkage_dataset3.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(dataset):\n",
    "    sm = np.array([np.square(dataset - dataset[i]).sum(axis=1) for i in range(len(dataset))])\n",
    "    sm[np.identity(len(sm)) == 1] = np.nan\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(distance_matrix, clusters, len_dm):\n",
    "    nanargmin_dm = np.nanargmin(distance_matrix)\n",
    "    min_i, min_j = nanargmin_dm // len_dm, nanargmin_dm % len_dm\n",
    "    cluster_min_i, cluster_min_j = clusters[min_i], clusters[min_j]\n",
    "    if not cluster_min_i == cluster_min_j:\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:, cluster_eq_j] = np.nan\n",
    "        distance_matrix[cluster_eq_i] = temp_dm    \n",
    "        temp_dm = distance_matrix[cluster_eq_j]\n",
    "        temp_dm[:, cluster_eq_i] = np.nan\n",
    "        distance_matrix[cluster_eq_j] = temp_dm\n",
    "        clusters[cluster_eq_j] = cluster_min_i\n",
    "        cluster_eq_i = clusters == cluster_min_i\n",
    "        cluster_eq_j = clusters == cluster_min_j\n",
    "        temp_dm = distance_matrix[cluster_eq_i]\n",
    "        temp_dm[:] = np.mean(temp_dm, axis=0)\n",
    "        distance_matrix[cluster_eq_i] = temp_dm\n",
    "        temp_dm = distance_matrix[:,cluster_eq_i]\n",
    "        temp_dm[:] = np.expand_dims(np.mean(temp_dm, axis=1), axis=1)\n",
    "        distance_matrix[:,cluster_eq_i] = temp_dm\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "    return distance_matrix, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_average(dataset, nb_of_clusters, dataname=\"data\", create_anim_file=False, plot_every_iter=15, print_output=True):\n",
    "    start_time = time.time()\n",
    "    dm = compute_distance_matrix(dataset)\n",
    "    clusters = np.arange(len(dataset))\n",
    "    len_dm = len(dm)\n",
    "    \n",
    "    for i in range(len(dataset) - nb_of_clusters):\n",
    "        dm, clusters = merge_clusters(dm, clusters, len_dm)\n",
    "        \n",
    "        unique_clusters = np.unique(clusters)\n",
    "        if create_anim_file and (i % plot_every_iter == 0 or len(unique_clusters) < 8):\n",
    "            if len(unique_clusters) < 8:\n",
    "                color_arr = [color_list[np.where(unique_clusters == cluster)[0][0]] for cluster in clusters]\n",
    "            else:\n",
    "                color_arr = [hsv_colors(cluster/len(dataset)) for cluster in clusters]\n",
    "            plt.scatter(dataset[:,0], dataset[:,1], c=color_arr)\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/groupaverage/{dataname}/{i:04d}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    squared_errors = 0\n",
    "    for i in unique_clusters:\n",
    "        cluster_centroid = dataset[clusters == i].mean(axis=0)\n",
    "        squared_error = np.square(dataset[clusters == i] - cluster_centroid).sum()\n",
    "        squared_errors += squared_error\n",
    "        \n",
    "    end_time = time.time()\n",
    "    if print_output:\n",
    "        print(f\"Sum of squared errors for {dataname} (normalized) with group average:{squared_errors:.4f}\")\n",
    "        print(f\"Group average for {dataname} took :{end_time - start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anim_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_average(dataset1, 7, dataname=\"dataset1\", create_anim_file=create_anim_file, plot_every_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_average(dataset2, 3, dataname=\"dataset2\", create_anim_file=create_anim_file, plot_every_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_average(dataset3, 2, dataname=\"dataset3\", create_anim_file=create_anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/groupaverage_dataset1.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/groupaverage/dataset1/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/groupaverage_dataset1.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/groupaverage_dataset2.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/groupaverage/dataset2/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/groupaverage_dataset2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/groupaverage_dataset3.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/groupaverage/dataset3/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(10):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=8)\n",
    "    \n",
    "anim_file = 'gif/groupaverage_dataset3.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(dataset, min_pts, eps, create_anim_file=False, dataname=\"data\"):\n",
    "    start_time = time.time()\n",
    "    eps_c = eps * eps\n",
    "\n",
    "    core_point_list = []\n",
    "    for point_index, cur_point in enumerate(dataset):\n",
    "        points_within_eps = 0\n",
    "        for other_point_index, other_point in enumerate(dataset):\n",
    "            if point_index == other_point_index:\n",
    "                continue\n",
    "            if np.square(cur_point - other_point).sum() < eps_c:\n",
    "                points_within_eps += 1\n",
    "            if points_within_eps >= min_pts:\n",
    "                core_point_list.append(point_index)\n",
    "                break\n",
    "\n",
    "    core_points = dataset[core_point_list]\n",
    "    non_core_points = np.delete(dataset, core_point_list, axis=0)\n",
    "\n",
    "    border_point_list = []\n",
    "    for non_core_point_index, non_core_point in enumerate(non_core_points):\n",
    "        for core_point in core_points:\n",
    "            if np.square(non_core_point - core_point).sum() < eps_c:\n",
    "                border_point_list.append(non_core_point_index)\n",
    "                break\n",
    "\n",
    "    border_points = non_core_points[border_point_list]\n",
    "    noise_points = np.delete(non_core_points, border_point_list, axis=0)\n",
    "\n",
    "    cur_cluster = -1\n",
    "    clusters = np.full(len(dataset), cur_cluster)\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    \n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "         \n",
    "        if create_anim_file and len(unique_clusters) <= 8:\n",
    "            color_arr = [color_list[cluster] for cluster in clusters]\n",
    "            plt.scatter(dataset[:,0], dataset[:,1], c=color_arr)\n",
    "            plt.title(f'{dataname}')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.savefig(f'gif/dbscan/{dataname}/{i:04d}.png')\n",
    "            plt.close()\n",
    "        \n",
    "        \n",
    "        occurence = np.nonzero(clusters[core_point_list] == -1)\n",
    "        if len(occurence[0]) > 0:\n",
    "            core_point_occurence = occurence[0][0]\n",
    "            cur_cluster += 1\n",
    "            clusters[core_point_list[core_point_occurence]] = cur_cluster\n",
    "\n",
    "            core_point = dataset[core_point_list[core_point_occurence]]\n",
    "            other_core_indices = []\n",
    "            for other_point_index, other_point in enumerate(dataset):\n",
    "                if clusters[other_point_index] == -1 and np.square(core_point - other_point).sum() < eps_c:\n",
    "                    clusters[other_point_index] = clusters[core_point_list[core_point_occurence]]\n",
    "                    if other_point_index in core_point_list:\n",
    "                        other_core_indices.append(other_point_index)\n",
    "            for other_core_index in other_core_indices:\n",
    "                new_core_point = dataset[other_core_index]\n",
    "                for other_point_index, other_point in enumerate(dataset):\n",
    "                    if clusters[other_point_index] == -1 and np.square(new_core_point - other_point).sum() < eps_c:\n",
    "                        clusters[other_point_index] = clusters[other_core_index]\n",
    "                        if other_point_index in core_point_list:\n",
    "                            other_core_indices.append(other_point_index)\n",
    "        else:\n",
    "            done = True\n",
    "        \n",
    "        i += 1\n",
    "            \n",
    "    unique_clusters = np.unique(clusters)\n",
    "    squared_errors = 0\n",
    "    for i in unique_clusters:\n",
    "        if i == -1:\n",
    "            continue\n",
    "        cluster_centroid = dataset[clusters == i].mean(axis=0)\n",
    "        squared_error = np.square(dataset[clusters == i] - cluster_centroid).sum()\n",
    "        squared_errors += squared_error\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Sum of squared errors for {dataname} (normalized) with DBSCAN:{squared_errors:.4f}\")\n",
    "    print(f\"DBSCAN for {dataname} took :{end_time - start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anim_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan(dataset1, min_pts=230, eps=0.795, create_anim_file=create_anim_file, dataname=\"dataset1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan(dataset2, min_pts=30, eps=0.4, create_anim_file=create_anim_file, dataname=\"dataset2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan(dataset3, min_pts=45, eps=0.927, create_anim_file=create_anim_file, dataname=\"dataset3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/dbscan_dataset1.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/dbscan/dataset1/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(3):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=2)\n",
    "    \n",
    "anim_file = 'gif/dbscan_dataset1.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/dbscan_dataset2.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/dbscan/dataset2/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(3):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=2)\n",
    "    \n",
    "anim_file = 'gif/dbscan_dataset2.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_anim_file:\n",
    "    anim_file = 'gif/dbscan_dataset3.gif'\n",
    "\n",
    "    frames = []\n",
    "    filenames = glob.glob('gif/dbscan/dataset3/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frames.append(imageio.imread(filename))\n",
    "    for i in range(3):\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimsave(anim_file, frames, 'GIF', fps=2)\n",
    "    \n",
    "anim_file = 'gif/dbscan_dataset3.gif'\n",
    "display.Image(filename=anim_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
